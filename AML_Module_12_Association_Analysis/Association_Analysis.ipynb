{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# Association Analysis\n",
                "\n",
                "**Author: Jessica Cervi**\n",
                "\n",
                "**Expected time =2 hours**\n",
                "\n",
                "**Total points = 55 points**\n",
                "\n",
                "\n",
                "## Assignment Overview\n",
                "\n",
                "In this assignment, we will work on association analysis. \n",
                "\n",
                "As we have seen, there are many complex ways to analyze data (clustering, regression, Neural Networks, Random Forests, SVM, etc.). The challenge with many of these approaches is that they can be difficult to tune, challenging to interpret and require quite a bit of data prep and feature engineering to get good results. In other words, they can be very powerful but require a lot of knowledge to implement properly.\n",
                "\n",
                "Association analysis is relatively light on math concepts and easy to explain to non-technical people. In addition, it is an unsupervised learning tool that looks for hidden patterns, so there is limited need for data prep and feature engineering. It is a good start for certain cases of data exploration and can point the way for a deeper dive into the data using other approaches.\n",
                "\n",
                "For this assignment we will use the Python implementation of association analysis found in the `MLxtend` library. This implementation should be very familiar to anyone who has exposure to `scikit-learn` and `pandas`. \n",
                "\n",
                "**NOTE:** Technically, market basket analysis is just one application of association analysis. In this assignment though, we will use association analysis and market basket analysis terms interchangeably.\n",
                "\n",
                "This assignment is designed to build your familiarity and comfort in coding in Python. It will also help you review the key topics from each module. As you progress through the assignment, answers to the questions will get increasingly complex. You must adopt a data scientist's mindset when completing this assignment. Remember to run your code from each cell before submitting your assignment. Running your code beforehand will notify you of errors and giving you a chance to fix your errors before submitting it. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
                "\n",
                "***Vocareum Tips***\n",
                "- Do not add arguments or options to functions unless asked specifically. This will cause an error in Vocareum.\n",
                "- Do not use a library unless you are explicitly asked in the question. \n",
                "- You can download the Grading Report after submitting the assignment. It will include the feedback and hints on incorrect questions. \n",
                "\n",
                "### Learning Objectives\n",
                "\n",
                "- Understanding the basic of association analysis\n",
                "- Apply one-hot encoding to a dataframe\n",
                "- Apply association analysis to data using the library `Mlxtend`\n",
                "- Interpret the results\n",
                "\n",
                "## Index: \n",
                "\n",
                "#### Association Analysis\n",
                "+ [Question 01](#q1)\n",
                "+ [Question 02](#q2)\n",
                "+ [Question 03](#q3)\n",
                "+ [Question 04](#q4)\n",
                "+ [Question 05](#q5)\n",
                "+ [Question 06](#q6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Association Analysis\n",
                "\n",
                "\n",
                "Association rules are normally written like this: {Bread} -> {Beer} which means that there is a strong relationship between customers that purchased bread and also purchased beer in the same transaction.\n",
                "\n",
                "In the above example, the {Bread} is the **antecedent** and the {Beer} is the **consequent**. Both antecedents and consequents can have multiple items. In other words, {Bread, Gum} -> {Beer, Chips} is also a valid rule.\n",
                "\n",
                "**Support** is the relative frequency that the rules show up. In many instances, you may want to look for high support in order to make sure it is a useful relationship. However, there may be instances where a low support is useful if you are trying to find \u201chidden\u201d relationships.\n",
                "\n",
                "**Confidence** is a measure of the reliability of the rule. A confidence of .5 in the above example would mean that in 50% of the cases where Bread and Gum were purchased, the purchase also included Beer and Chips. For product recommendation, a 50% confidence may be perfectly acceptable but in a medical situation, this level may not be high enough.\n",
                "\n",
                "**Lift** is the ratio of the observed support to that expected if the two rules were independent (click [here](https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning) for more details). The basic rule of thumb is that a lift value close to 1 means the rules were completely independent. Lift values greater than 1 are generally more \u201cinteresting\u201d and could be indicative of a useful rule pattern.\n",
                "\n",
                "One final note, related to the data. This analysis requires that all the data for a transaction be included in 1 row and the items should be 1-hot encoded. \n",
                "\n",
                "You can find the documentation abut the `MLxtend` library [here](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/frequent_patterns\/apriori\/).\n",
                "\n",
                "\n",
                "### Apriori algorithm\n",
                "The apriori algorithm is commonly used for association analysis in scenarios like e-commerce. The objective is to find which items are commonly bought toghether.\n",
                "\n",
                "Since big e-commerce sites usually have tens of thousands of different items and millions of different historical orders, an exhaustive brute-force search of which items are commonly bought together is unfeasible.\n",
                "\n",
                "The **Apriori** algorithm allows us to find all the combinations of _k_ items that appear at least in _t_ different orders. The computation is much faster than brute-force approaches because the algorithm simplifies the search by recursive elimination of item combinations that do not satisfy the search constraints.\n",
                "\n",
                "The recursive elimination of item combinations is illustrated in the image below:\n",
                "\n",
                "<img src=\"data\/combination-graph.png\">\n",
                "\n",
                "The image represents a scenario with 5 different items _a,b,c,d,e_, and we need to find the combinations of such items up to length _k_ with a threshold _t_. In this example the combination _ab_ is not present in at least _t_ orders and it is therefore discarded. Since _ab_ does not satisfy the threshold, any other itemset containing _ab_ will also be discarded. The combinations are computed adding one extra element at a time (until _k_ elements) and the search space is iteratively reduced by just keeping these combinations that do not satisfy the threshold."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Importing the dataset and exploratory data analysis\n",
                "\n",
                "For this assignment, we will be using  data that comes from the UCI Machine Learning Repository. This data represents transactional data from a UK retailer from 2010-2011. This mostly represents sales to wholesalers, so it is slightly different from consumer purchase patterns but is still a useful case study.\n",
                "\n",
                "You can find more information about the dataset [here](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail).\n",
                "\n",
                "Below we import the necessary libraries and read the dataset using the `pandas` `read_excel` function."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from mlxtend.frequent_patterns import apriori\n",
                "from mlxtend.frequent_patterns import association_rules\n",
                "\n",
                "df = pd.read_excel('data\/Online_retail.xlsx')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we use the function `head()` to visualize the first five rows of our dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Here's a description of the attributes in our dataframe:\n",
                "\n",
                "- InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
                "- StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. \n",
                "- Description: Product (item) name. Nominal. \n",
                "- Quantity: The quantities of each product (item) per transaction. Numeric.\t\n",
                "- InvoiceDate: Invoice Date and time. Numeric, the day and time when each transaction was generated. \n",
                "- UnitPrice: Unit price. Numeric, Product price per unit in sterling. \n",
                "- CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. \n",
                "- Country: Country name. Nominal, the name of the country where each customer resides.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "[Back to top](#Index:) \n",
                "<a id='q1'><\/a>\n",
                "\n",
                "### Question 1:\n",
                "\n",
                "*5 points*\n",
                " \n",
                "For the sake of keeping the data set small, I\u2019m only looking at sales for France.\n",
                " \n",
                "Create a subset of our dataframe containing only the orders from `France`. Assign the new dataframe to `df_france`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "df_france = None\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 01",
                    "locked": true,
                    "points": "5",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first thing we need to do is to consolidate the items into 1 transaction per row with each product **one hot encoded**. \n",
                "\n",
                "\n",
                "[Back to top](#Index:) \n",
                "<a id='q2'><\/a>\n",
                "\n",
                "### Question 2:\n",
                "\n",
                "*10 points*\n",
                "\n",
                "Group the entries of `df_france` by `InvoiceNo` and `Description` into a series called `basket`. Each entry of `basket` should be the sum of how many times a certain product appears over all the transactions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "basket = None\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 02",
                    "locked": true,
                    "points": "10",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[Back to top](#Index:) \n",
                "<a id='q3'><\/a>\n",
                "\n",
                "### Question 3:\n",
                "\n",
                "*10 points*\n",
                "\n",
                "Unstack the series `basket` to create a dataframe `df_basket`. Do so by using the function `unstack()`.\n",
                "\n",
                "Next, reset the index of your dataframe, fill the NaN values with zeroes, and set the index column to `InvoiceNo`.\n",
                "\n",
                "**HINT: The functions `reset_index()`, `fillna()` and `set_index()` will be useful**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": false,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "df_basket = None\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 03",
                    "locked": true,
                    "points": "10",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's have a look at our new dataframe!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "df_basket"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "There are a lot of zeros in the data. We also need to convert any positive values to a 1 and convert any values less than 0 to a 0.\n",
                "\n",
                "[Back to top](#Index:) \n",
                "<a id='q4'><\/a>\n",
                "\n",
                "### Question 4:\n",
                "\n",
                "*10 points*\n",
                "\n",
                "Define a function, `encode_units` that takes one float, `x`. Your function should return `0` if `x` is less than or equal to zero or 1 if `x` is greater than or equal to one.\n",
                "\n",
                "Use the function `applymap()` to apply this function to your dataframe `df_basket`. Assign the new dataframe to `basket_sets`.\n",
                "\n",
                "Finally, remove the `POSTAGE` column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "basket_sets = None\n",
                "\n",
                "def encode_unit():\n",
                "    return\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 04",
                    "locked": true,
                    "points": "10",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Now that the data is structured properly, we can generate frequent item sets that have a support of at least 7%.\n",
                "\n",
                "[Back to top](#Index:) \n",
                "<a id='q5'><\/a>\n",
                "\n",
                "### Question 5:\n",
                "\n",
                "*10 points*\n",
                "\n",
                "Use the function `apriori` from the `mlxtend` library on `basket_sets` to create a dataframe `frequent_itemsets` with the support for each item. Set the arguments of `apriori` to `min_support = 0.07` and `use_colnames = True`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "\n",
                "frequent_itemsets = None\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 05",
                    "locked": true,
                    "points": "10",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's have a look at our result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "frequent_itemsets.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The final step is to generate the rules with their corresponding support, confidence, and lift. We can do so using the function `association_rules` from `mlxtend`.\n",
                "\n",
                "[Back to top](#Index:) \n",
                "<a id='q6'><\/a>\n",
                "\n",
                "### Question 6:\n",
                "\n",
                "*10 points*\n",
                "\n",
                "Use the function `association_rules` from `mlxtend` to create the decided dataframe from `frequent_itemsets`. Name this dataframe `rules`. Set the arguments `metric= \"lift\"`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "### GRADED\n",
                "\n",
                "### YOUR SOLUTION HERE\n",
                "\n",
                "rules = None\n",
                "\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Question 06",
                    "locked": true,
                    "points": "10",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's have a look at our result!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rules.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, the tricky part is figuring out what this tells us. For instance, we can see that there are quite a few rules with a high lift value, which means that it occurs more frequently than it would be expected given the number of transactions and product combinations. We can also see several rules where the confidence is high. This part of the analysis is where the domain knowledge will come in handy. Since we do not have that, we\u2019ll just look for a couple of illustrative examples.\n",
                "\n",
                "We can filter the dataframe using standard `pandas` code. In this case, look for a large lift (6) and high confidence (.8), like so:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rules[ (rules['lift'] >= 6) & (rules['confidence'] >= 0.8) ]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What is also interesting to see is how the combinations vary by the country of purchase. Let\u2019s check out what some popular combinations might be in Germany. \n",
                "\n",
                "We can do so by creating a new subset of our dataframe (with `Germany` set as a counter) and by applying the same steps as above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "basket2 = (df[df['Country'] ==\"Germany\"]\n",
                "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
                "          .sum().unstack().reset_index().fillna(0)\n",
                "          .set_index('InvoiceNo'))\n",
                "\n",
                "basket_sets2 = basket2.applymap(encode_units)\n",
                "basket_sets2.drop('POSTAGE', inplace=True, axis=1)\n",
                "frequent_itemsets2 = apriori(basket_sets2, min_support=0.05, use_colnames=True)\n",
                "rules2 = association_rules(frequent_itemsets2, metric=\"lift\")\n",
                "\n",
                "rules2[ (rules2['lift'] >= 4) &\n",
                "        (rules2['confidence'] >= 0.5)]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It seems that in addition to David Hasselhoff, Germans love Plasters in Tin Spaceboy and Woodland Animals.\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}